{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "013caf14",
   "metadata": {},
   "source": [
    "### I. List of Professor ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f4d84f",
   "metadata": {},
   "source": [
    "overall rating, symnonym (Manu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcb2446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = '''Marville wasn't involved in the lab section of this class at all and only came in \n",
    "on occasion to pick at dress code violations. The TA did everything. Having a good TA and \n",
    "rewatching the pre-lab videos a lot is going to be very important if you want to pass the class.'''\n",
    "rate1 = '''My fav professor at UIUC! She responds to email very quickly. \n",
    "Once I sent her a question on the Bohr model and she replied to my email within 10 mins.\n",
    "Her way of teaching helps me retain information a lot more effectively. \n",
    "Her class is tough, but she makes sure that students understand.'''\n",
    "rate2 = '''Was the lab supervisor for chem 103 lab. Totally on a power trip and incredibly \n",
    "rude towards not only students but also visibly hostile toward TAs. Glad she did not come \n",
    "into my section's lab often however when she did she would act like a tyrant over minute details \n",
    "to show she has power. Totally unprofessional and needs to be looked into.'''\n",
    "rate3 = '''I hate her, she doesn't know how to curve tests, is happy with a 60% average,\n",
    "and gives almost no free assignments to compensate for the lack of curve. Stay away.'''\n",
    "rate4 = '''The class is hellish at times but I got an A in the class with zero chem experience \n",
    "(not even in high school). Marville states in the syllabus that she expects her students to put in \n",
    "a lot of effort which frankly is the reality of what a college class should be like. \n",
    "I really wish she curved grades though.'''\n",
    "rates = rate, rate1, rate2, rate3, rate4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1185b934",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(\"subjective_eval.txt\").read().splitlines()\n",
    "print(lines) #read subjective_eval.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25da7a1",
   "metadata": {},
   "source": [
    "### II. NLTK sentiment Intensity Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b3f4f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c60e7701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall, the sentiment dictionary is: {'neg': 0.061, 'neu': 0.826, 'pos': 0.113, 'compound': 0.2247}.\n",
      "The text is 11.3% positive.\n",
      "The text is 82.6% neutral.\n",
      "The text is 6.1% negative.\n",
      "The sentiment of this text is mainly POSITIVE.\n"
     ]
    }
   ],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "sentiment_dict = sia.polarity_scores(rate)\n",
    "print(f'Overall, the sentiment dictionary is: {sentiment_dict}.')\n",
    "print(f'The text is {sentiment_dict[\"pos\"]:.1%} positive.')\n",
    "print(f'The text is {sentiment_dict[\"neu\"]:.1%} neutral.')\n",
    "print(f'The text is {sentiment_dict[\"neg\"]:.1%} negative.')\n",
    "\n",
    "if sentiment_dict['compound'] >= 0.05:\n",
    "    print('The sentiment of this text is mainly POSITIVE.')\n",
    "elif sentiment_dict['compound'] <= - 0.05:\n",
    "    print('The sentiment of this text is mainly NEGATIVE.')\n",
    "else:\n",
    "    print('The sentiment of this text is mainly NEUTRAL.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e34027b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c981c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens from ratings = ['was', 'the', 'lab', 'supervisor', 'for', 'chem', '103', 'lab', 'totally', 'on', 'a', 'power', 'trip', 'and', 'incredibly', 'rude', 'towards', 'not', 'only', 'students', 'but', 'also', 'visibly', 'hostile', 'toward', 'tas', 'glad', 'she', 'did', 'not', 'come', 'into', 'my', 'section', 's', 'lab', 'often', 'however', 'when', 'she', 'did', 'she', 'would', 'act', 'like', 'a', 'tyrant', 'over', 'minute', 'details', 'to', 'show', 'she', 'has', 'power', 'totally', 'unprofessional', 'and', 'needs', 'to', 'be', 'looked', 'into']\n",
      "\n",
      "Tokens from filtered ratings = ['lab', 'supervisor', 'chem', '103', 'lab', 'totally', 'power', 'trip', 'incredibly', 'rude', 'towards', 'students', 'also', 'visibly', 'hostile', 'toward', 'tas', 'glad', 'come', 'section', 'lab', 'often', 'however', 'would', 'act', 'like', 'tyrant', 'minute', 'details', 'show', 'power', 'totally', 'unprofessional', 'needs', 'looked']\n"
     ]
    }
   ],
   "source": [
    "rate_alphanum = re.sub(\"[^a-zA-Z0-9]\", \" \", rate2)\n",
    "rate_alphanum = rate_alphanum.lower()\n",
    "rate_tokens = nltk.word_tokenize(rate_alphanum)\n",
    "print(\"Tokens from ratings =\", rate_tokens)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "rate_filtered = [word for word in rate_tokens if not word in stop_words]\n",
    "print(\"\\nTokens from filtered ratings =\", rate_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f46a7ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEMMING\n",
      "---All stems:  ['lab', 'supervisor', 'chem', '103', 'lab', 'total', 'power', 'trip', 'incred', 'rude', 'toward', 'student', 'also', 'visibl', 'hostil', 'toward', 'ta', 'glad', 'come', 'section', 'lab', 'often', 'howev', 'would', 'act', 'like', 'tyrant', 'minut', 'detail', 'show', 'power', 'total', 'unprofession', 'need', 'look'] \n",
      "\n",
      "---Most common stems:  [('lab', 3), ('total', 2), ('power', 2), ('toward', 2), ('supervisor', 1), ('chem', 1), ('103', 1), ('trip', 1), ('incred', 1), ('rude', 1), ('student', 1), ('also', 1), ('visibl', 1), ('hostil', 1), ('ta', 1), ('glad', 1), ('come', 1), ('section', 1), ('often', 1), ('howev', 1)]\n",
      "\n",
      "LEMMATIZING\n",
      "---All lemmas:  ['lab', 'supervisor', 'chem', '103', 'lab', 'totally', 'power', 'trip', 'incredibly', 'rude', 'towards', 'student', 'also', 'visibly', 'hostile', 'toward', 'ta', 'glad', 'come', 'section', 'lab', 'often', 'however', 'would', 'act', 'like', 'tyrant', 'minute', 'detail', 'show', 'power', 'totally', 'unprofessional', 'need', 'looked']\n",
      "\n",
      "---Most commonlemmas:  [('lab', 3), ('totally', 2), ('power', 2), ('supervisor', 1), ('chem', 1), ('103', 1), ('trip', 1), ('incredibly', 1), ('rude', 1), ('towards', 1), ('student', 1), ('also', 1), ('visibly', 1), ('hostile', 1), ('toward', 1), ('ta', 1), ('glad', 1), ('come', 1), ('section', 1), ('often', 1)]\n"
     ]
    }
   ],
   "source": [
    "# stemming\n",
    "print(\"STEMMING\")\n",
    "stemmer = PorterStemmer()\n",
    "rate_stems = list(map(stemmer.stem, rate_filtered))\n",
    "print(\"---All stems: \", rate_stems, \"\\n\")\n",
    "\n",
    "stems = Counter(rate_stems).most_common(20)\n",
    "print(\"---Most common stems: \", stems)\n",
    "    \n",
    "# lemmatizing\n",
    "print(\"\\nLEMMATIZING\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "rate_lemmas = list(map(lemmatizer.lemmatize, rate_filtered))\n",
    "print(\"---All lemmas: \", rate_lemmas)\n",
    "\n",
    "lemmas = Counter(rate_lemmas).most_common(20)\n",
    "print(\"\\n---Most commonlemmas: \", lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fd5ace",
   "metadata": {},
   "source": [
    "### III. Define function AnalyzeSentimentOfProfRating(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f6a75e",
   "metadata": {},
   "source": [
    "SentimentIntensityAnalyzer compute the sentiment scores of the input rating, which include positive, negative, and neutral scores, as well as an overall sentiment score called the compound score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd6319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AnalyzeSentimentOfProfRating(r):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_dict = sia.polarity_scores(r)\n",
    "    print(f'Overall, the sentiment dictionary is: {sentiment_dict}.')\n",
    "    print(f'The text is {sentiment_dict[\"pos\"]:.1%} positive.')\n",
    "    print(f'The text is {sentiment_dict[\"neu\"]:.1%} neutral.')\n",
    "    print(f'The text is {sentiment_dict[\"neg\"]:.1%} negative.')\n",
    "    if sentiment_dict['compound'] >= 0.05:\n",
    "        print('The sentiment of this text is mainly positive.')\n",
    "    elif sentiment_dict['compound'] <= - 0.05:\n",
    "        print('The sentiment of this text is mainly negative.')\n",
    "    else:\n",
    "        print('The sentiment of this text is mainly neutral.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0b6486",
   "metadata": {},
   "source": [
    "### IV. K-means clustering with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f244e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# vectorize the text data (Convert a collection of raw documents to a matrix of TF-IDF features)\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(rates) # transform the 'rates' data into a matrix of TF-IDF features\n",
    "\n",
    "# cluster the data\n",
    "n_init = 10\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# print the cluster assignments\n",
    "for i, rate in enumerate(rates):\n",
    "    print(rate)\n",
    "    print(\"-->Kmeans labels\", kmeans.labels_[i], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66bf85b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for rating in rates:\n",
    "    print(rating, \"\\n\")\n",
    "    AnalyzeSentimentOfProfRating(rating)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c322f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# reduce the dimensionality of the TF-IDF features using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X.toarray())\n",
    "\n",
    "# create a scatter plot of the PCA-reduced data, colored by cluster assignment\n",
    "colors = ['r', 'g', 'b'] # one color for each cluster\n",
    "for i in range(len(rates)):\n",
    "    plt.scatter(X_pca[i,0], X_pca[i,1], s=200, alpha=0.5, color=colors[kmeans.labels_[i]])\n",
    "    plt.text(X_pca[i,0], X_pca[i,1], rates[i], fontsize=8)\n",
    "\n",
    "# add labels to the plot\n",
    "plt.title('K-means Clustering Results')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76be19e6",
   "metadata": {},
   "source": [
    "Heatmap, 1D graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7109ddd7",
   "metadata": {},
   "source": [
    "### Interpretation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5401ad",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) is a technique for dimensionality reduction that focuses on identifying the directions with the most variance in the data, and projecting the data onto those directions.\n",
    "\n",
    "- In PCA, the concept of variance refers to the spread or variability of data points along a certain axis or direction in the feature space. The direction with the most variance in the data represents the principal component 1 (PC1), while the direction with the second most variance represents the principal component 2 (PC2), and so on.\n",
    "\n",
    "- The direction with the most variance is considered to be the most important direction in the feature space because it captures the largest amount of information about the data. In other words, it represents the most significant pattern or trend in the data. Principal components with lower variances represent less important or less significant patterns in the data.\n",
    "\n",
    "- The possible meaning of principal component 1 and 2 is not related to the professor reviews themselves, but rather to the underlying patterns in the data. Principal component 1 represents the direction in the feature space that explains the most variance in the data, while principal component 2 represents the direction that explains the second most variance. The position of each review in the PCA plot reflects the degree of similarity between the reviews in terms of their TF-IDF feature values. Reviews that are closer together in the plot are more similar in their feature values, while reviews that are farther apart are more dissimilar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b5b858",
   "metadata": {},
   "source": [
    "The number on the axis of a principal component represents the weight or importance of each original feature in the principal component. The larger the absolute value of a weight, the more important that feature is in determining the values along that principal component. It is important to note that the weights are normalized, meaning that they represent the relative importance of each feature in the principal component, not their absolute importance.\n",
    "\n",
    "In a principal component 1 and 2 graph, the sign (positive or negative) of a data point along each axis represents its position relative to the mean of the data. For example, if a data point is located to the right of the mean along the principal component 1 axis, it will have a positive value on that axis. Conversely, if a data point is located to the left of the mean, it will have a negative value on the principal component 1 axis. The same applies for the principal component 2 axis. In general, the sign of a data point along each axis doesn't have a specific meaning on its own. Rather, it's the overall position of the data point in the feature space that provides insight into its characteristics.\n",
    "\n",
    "For example data point with (0.8, -0.8) has a positive value along the first principal component and a negative value along the second principal component. This means that the data point has a strong positive correlation with the features that contribute most to the first principal component and a strong negative correlation with the features that contribute most to the second principal component. However, the magnitude of the values alone does not necessarily indicate the importance of a feature in determining the values along a principal component. The importance of a feature is determined by its contribution to the overall variance of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1db1c8",
   "metadata": {},
   "source": [
    "Weighting vs Normalizing:\n",
    "- Weighting: the data means assigning a weight to each data point based on its importance or relevance to the analysis. \n",
    "\n",
    "- Normalizing: the data to have a mean of zero and a standard deviation of one is a common normalization technique, but it is not the only way to normalize data. Normalization is simply the process of transforming data so that it falls within a specific range or distribution --> different features are on the same scale and can be more easily compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e198fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# count the number of reviews in each cluster\n",
    "counts = np.bincount(kmeans.labels_)\n",
    "\n",
    "# create a list of tuples containing the cluster label and its count\n",
    "cluster_sizes = [(i, count) for i, count in enumerate(counts)]\n",
    "\n",
    "# sort the list by cluster size in descending order\n",
    "cluster_sizes.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# create a list of colors for the bubbles, one color for each cluster\n",
    "colors = ['r', 'g', 'b']\n",
    "\n",
    "# create a scatter plot of the cluster sizes as bubbles\n",
    "for i, (cluster, size) in enumerate(cluster_sizes):\n",
    "    plt.scatter(i, 0, s=size*300, alpha=0.5, color=colors[cluster])\n",
    "    plt.text(i, 0, f'Cluster {cluster}: {size} reviews', ha='center', va='center')\n",
    "\n",
    "# add labels and formatting to the plot\n",
    "plt.title('K-means Clustering Results')\n",
    "plt.xlabel('Cluster')\n",
    "plt.xticks(range(len(cluster_sizes)), [f'Cluster {cluster}' for cluster, size in cluster_sizes], rotation=90)\n",
    "plt.yticks([])\n",
    "plt.xlim(-0.5, len(cluster_sizes)-0.5)\n",
    "plt.ylim(-1, 1)\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e3173f",
   "metadata": {},
   "source": [
    "### In the context of the code you provided, PCA is used to transform the high-dimensional TF-IDF matrix into a lower-dimensional 2D space that can be easily visualized using a scatter plot. This allows us to see the cluster assignments of each data point in a two-dimensional space, rather than in the high-dimensional feature space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae3355b",
   "metadata": {},
   "source": [
    "### VI. Topic modeling using Gensim library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513a2828",
   "metadata": {},
   "source": [
    "How it works here: \n",
    "\n",
    "The LDA model is designed to work with a corpus of text, which can consist of multiple documents. In this case, each rating from a different student can be treated as a separate document in the corpus.\n",
    "\n",
    "It tokenizes the input strings and creates a document-term matrix, which represents the frequency of each word in each document. This matrix can be used as input to the LDA model to extract the latent topics present in the ratings.\n",
    "\n",
    "After training the LDA model, we can examine the resulting topic distributions for each document (i.e., each rating), to see which topics are most strongly associated with each rating. This can provide insights into the COMMON THEMES AND CONCERNS that are present in the students' ratings of the professor, and can help identify areas where the professor is performing well or where improvement is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d0860c",
   "metadata": {},
   "source": [
    "For example, suppose that you have a corpus of documents, and you train an LDA model with 5 topics. After training the model, you can examine the topic distributions for each document. For a given document, the topic distribution might look something like this:\n",
    "Topic 1: 0.20\n",
    "Topic 2: 0.10\n",
    "Topic 3: 0.60\n",
    "Topic 4: 0.05\n",
    "Topic 5: 0.05\n",
    "In this example, the document is most strongly associated with Topic 3, with a probability of 0.60. This means that 60% of the document's words were assigned to Topic 3 by the LDA model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fec761",
   "metadata": {},
   "outputs": [],
   "source": [
    "rates2 = [    \"The professor was very engaging and made the material easy to understand.\",\n",
    "          \"I found the professor's lectures to be a bit dry and difficult to follow.\", \n",
    "          \"The professor was always available for questions and provided great feedback on assignments.\",\n",
    "          \"I appreciated the professor's enthusiasm and passion for the subject matter.\",\n",
    "          \"The professor seemed disorganized and often went off on tangents during lectures.\",\n",
    "          \"The professor was very knowledgeable and had a lot of interesting insights to share.\",\n",
    "          \"I thought the professor's assignments were challenging but fair.\",\n",
    "          \"The professor sometimes seemed impatient or dismissive of students' questions.\",\n",
    "          \"I found the professor's teaching style to be a bit old-fashioned and not very engaging.\",\n",
    "          \"Overall, I thought the professor did a good job and I learned a lot from the course.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7e1026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import corpora, models\n",
    "\n",
    "# define the stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# tokenize the input strings\n",
    "rate_alphanum = re.sub(\"[^a-zA-Z0-9]\", \" \", rate2)\n",
    "rate_tokens = nltk.word_tokenize(rate_alphanum.lower())\n",
    "\n",
    "# tokenize the input strings and remove stop words\n",
    "tokenized_statements = [[token for token in rate_tokens if token not in stop_words] for statement in rates]\n",
    "\n",
    "# create a dictionary from the tokenized statements\n",
    "dictionary = corpora.Dictionary(tokenized_statements)\n",
    "\n",
    "# create a document-term matrix\n",
    "corpus = [dictionary.doc2bow(statement) for statement in tokenized_statements]\n",
    "\n",
    "# train the LDA model\n",
    "lda = models.ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=10)\n",
    "\n",
    "# print the top words in each topic\n",
    "for i in range(5):\n",
    "    print(rates[i])\n",
    "    print(\"\\n--> Topic modelling: \", \"\\n\", lda.print_topic(i))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c042d68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import corpora, models\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from scipy.cluster import hierarchy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define the stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocess the text data\n",
    "rate_alphanum = [re.sub(\"[^a-zA-Z0-9]\", \" \", rate) for rate in rates2]\n",
    "rate_tokens = [nltk.word_tokenize(rate.lower()) for rate in rate_alphanum]\n",
    "tokenized_statements = [[token for token in rate if token not in stop_words] for rate in rate_tokens]\n",
    "\n",
    "# Create a dictionary from the tokenized statements\n",
    "dictionary = corpora.Dictionary(tokenized_statements)\n",
    "\n",
    "# Create a document-term matrix\n",
    "corpus = [dictionary.doc2bow(statement) for statement in tokenized_statements]\n",
    "\n",
    "# Train the LDA model\n",
    "lda = models.ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=10)\n",
    "\n",
    "# Print the top words in each topic\n",
    "for i in range(5):\n",
    "    print(\"\\nTopic modeling: \", \"index\", i , \" \", lda.print_topic(i))\n",
    "\n",
    "# Compute pairwise cosine distances between the topics\n",
    "distances = cosine_distances(lda.get_topics())\n",
    "\n",
    "# Compute the hierarchical clustering of the topics based on the cosine distances\n",
    "# The output of hierarchy.linkage() is a matrix of shape (n-1, 4), \n",
    "# where n is the number of topics. Each row of the matrix represents a merge of two clusters,\n",
    "# with the first two columns containing the indices of the two clusters that were merged,\n",
    "# the third column containing the distance between them, and the fourth column containing the size of the new cluster.\n",
    "\n",
    "model = hierarchy.linkage(distances, method='ward')\n",
    "print(\"\\n\", model)\n",
    "\n",
    "# Visualize the topic tree\n",
    "plt.figure(figsize=(10, 5))\n",
    "dendrogram = hierarchy.dendrogram(model, labels=range(lda.num_topics), leaf_font_size=8)\n",
    "plt.ylabel('Cosine distance', fontsize=12)\n",
    "plt.xlabel('Topic index', fontsize=12)\n",
    "plt.title('Topic Tree', fontsize=14)\n",
    "\n",
    "# Label the leaf nodes with the topic index\n",
    "ax = plt.gca()\n",
    "x_labels = ax.get_xmajorticklabels()\n",
    "for label in x_labels:\n",
    "    label.set_visible(False)\n",
    "for i, label in enumerate(x_labels):\n",
    "    if i % 1 == 0:  # Adjust the spacing between labels as needed\n",
    "        label.set_visible(True)\n",
    "    else:\n",
    "        label.set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fb80ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import corpora, models\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from scipy.cluster import hierarchy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Define the stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocess the text data\n",
    "rate_alphanum = [re.sub(\"[^a-zA-Z0-9]\", \" \", rate) for rate in rates2]\n",
    "rate_tokens = [nltk.word_tokenize(rate.lower()) for rate in rate_alphanum]\n",
    "tokenized_statements = [[token for token in rate if token not in stop_words] for rate in rate_tokens]\n",
    "\n",
    "# Create a dictionary from the tokenized statements\n",
    "dictionary = corpora.Dictionary(tokenized_statements)\n",
    "\n",
    "# Create a document-term matrix\n",
    "corpus = [dictionary.doc2bow(statement) for statement in tokenized_statements]\n",
    "\n",
    "# Train the LDA model\n",
    "lda = models.ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=10)\n",
    "\n",
    "# Assign labels to each topic based on top words\n",
    "topic_labels = [\"Interesting and \\n knowledgeable\",\n",
    "                \"Engaging Teaching \\n style\",\n",
    "                \"Challenging \\n assignments\",\n",
    "                \"Providing \\nFeedback\",\n",
    "                \"Impatient and \\n dismissive\"]\n",
    "\n",
    "# Print the top words in each topic\n",
    "for i in range(5):\n",
    "    print(\"\\nTopic modeling at index\", i, lda.print_topic(i))\n",
    "\n",
    "# Compute pairwise cosine distances between the topics\n",
    "distances = cosine_distances(lda.get_topics())\n",
    "\n",
    "# Compute the hierarchical clustering of the topics based on the cosine distances\n",
    "model = hierarchy.linkage(distances, method='ward')\n",
    "print(\"\\n\", model)\n",
    "\n",
    "# Visualize the topic tree\n",
    "plt.figure(figsize=(10, 5))\n",
    "dendrogram = hierarchy.dendrogram(model, labels=topic_labels, leaf_font_size=8) # or labels=range(lda.num_topics)\n",
    "plt.ylabel('Cosine distance', fontsize=12)\n",
    "plt.xlabel('Topic index', fontsize=12)\n",
    "plt.title('Topic Tree', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Calculate the sentiment scores for each statement using VADER\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sentiment_scores = [sia.polarity_scores(statement)['compound'] for statement in lines]\n",
    "\n",
    "# Plot the distribution of sentiment scores\n",
    "sns.histplot(sentiment_scores, bins=20, kde=False)\n",
    "plt.xlabel('Sentiment Score', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.title('Distribution of Sentiment Scores', fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52e76c0",
   "metadata": {},
   "source": [
    "/////////action word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62589540",
   "metadata": {},
   "source": [
    "Sentiment Scores is the array of the sentiment score of each data point in the same order as the original data array.\n",
    "\n",
    "A dendrogram is a type of tree diagram often used in hierarchical clustering to visualize the arrangement of the clusters. It shows how individual data points are grouped together into larger clusters based on their similarity. While dendrograms may look similar to classification trees, they are not the same thing. Classification trees are used in decision trees, which are a type of supervised machine learning algorithm used for predictive modeling.\n",
    "- Hierarchical clustering, also known as hierarchical cluster analysis, is an algorithm that groups similar objects into groups called clusters. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other.\n",
    "\n",
    "The dendrogram generated by the hierarchical clustering algorithm may vary each time the code is executed due to the fact that the algorithm uses a random initialization method. This means that the starting position and order of the data points can differ each time the algorithm is run, which can lead to slight variations in the resulting dendrogram. However, the overall structure of the dendrogram should remain consistent as long as the same data and parameters are used.\n",
    "\n",
    "\n",
    "The overall structure of the dendrogram and the groupings of clusters should remain consistent if the underlying data and clustering method are consistent. However, as the dendrogram is a visualization tool and the layout algorithm used to draw the dendrogram can vary between runs, the visual appearance of the dendrogram may vary slightly each time the code is run.\n",
    "\n",
    "- To ensure that the underlying structure of the dendrogram is consistent, you can check the groupings of clusters and the distances between them. If the groupings and distances are consistent between different runs of the code, then the overall structure of the dendrogram is likely to be consistent as well.\n",
    "\n",
    "- It is possible for the distances to vary slightly due to the random initialization of the algorithm and small differences in the data or parameters. However, if the distances and groupings vary significantly between runs, it may indicate instability or inconsistency in the clustering results. It is a good practice to perform multiple runs of the clustering algorithm and check for consistency in the groupings and distances.\n",
    "\n",
    "- The topic modeling approach used in the code above is unsupervised and does not infer any sentiment. It is simply identifying the underlying topics that are present in the text data based on the co-occurrence patterns of words. However, sentiment analysis can be combined with topic modeling to identify the sentiment associated with each topic.\n",
    "\n",
    "Hierarchical clustering is a technique used for unsupervised learning, which means that it does not require labeled data. The objective of hierarchical clustering is to group data points together based on similarity or distance measures, forming a hierarchy of clusters. The result of hierarchical clustering is a dendrogram, which shows the relationships between clusters and subclusters. Hierarchical clustering is useful for exploratory data analysis and identifying patterns in data.\n",
    "\n",
    "On the other hand, decision trees are a supervised learning technique, which means that they require labeled data to train the model. Decision trees are used to classify data by making a series of decisions based on a set of rules or conditions. Each decision results in the data being split into smaller subsets, until a decision is made for each subset. Decision trees are useful for classification tasks, where the goal is to assign each data point to a specific category or class.\n",
    "\n",
    "In summary, hierarchical clustering is a useful tool for exploratory data analysis and identifying patterns in data, while decision trees are useful for classification tasks where the goal is to assign each data point to a specific category or class based on a set of rules or conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2150b12e",
   "metadata": {},
   "source": [
    "### VI. Name Entity Recognition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32796114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d024ddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn_crfsuite import scorers\n",
    "\n",
    "# Define the list of reviews and their corresponding named entities\n",
    "lines = [(\"The food was great but the service was terrible.\", []),\n",
    "         (\"I had a wonderful experience at this restaurant.\", []),\n",
    "         (\"The hotel room was spacious and clean.\", []),\n",
    "         (\"I would not recommend this place to anyone.\", []),\n",
    "         (\"The staff was friendly and helpful.\", [])]\n",
    "\n",
    "# Define a function to extract features from each word in a sentence\n",
    "def word_features(sentence, i):\n",
    "    word = sentence[i][0]\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'pos': sentence[i][1],\n",
    "        'pos[:2]': sentence[i][1][:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sentence[i-1][0]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:pos': sentence[i-1][1],\n",
    "            '-1:pos[:2]': sentence[i-1][1][:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "    if i < len(sentence)-1:\n",
    "        word1 = sentence[i+1][0]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:pos': sentence[i+1][1],\n",
    "            '+1:pos[:2]': sentence[i+1][1][:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "    return features\n",
    "\n",
    "# Extract features from the review data\n",
    "X = [[word_features(line, i) for i in range(len(line))] for line in nltk.pos_tag_sents([nltk.word_tokenize(line) for line, _ in lines])]\n",
    "\n",
    "# Extract the named entities from the review data\n",
    "y = [[label for _, label in nltk.pos_tag(nltk.word_tokenize(line))] for line, _ in lines]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Train a CRF model on the training data\n",
    "crf = sklearn_crfsuite.CRF(algorithm='lbfgs', c1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f7d06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "# Tokenize the reviews\n",
    "tokenized_lines = [word_tokenize(line) for line in lines]\n",
    "\n",
    "# Tag the parts of speech for each word\n",
    "pos_lines = [pos_tag(line) for line in tokenized_lines]\n",
    "\n",
    "# Identify named entities using the NE Chunker\n",
    "chunked_lines = [ne_chunk(pos_line) for pos_line in pos_lines]\n",
    "\n",
    "# Extract the named entities from the chunked lines\n",
    "named_entities = []\n",
    "for chunked_line in chunked_lines:\n",
    "    for entity in chunked_line:\n",
    "        if hasattr(entity, 'label') and entity.label() == 'PERSON':\n",
    "            named_entities.append(' '.join([word for word, tag in entity.leaves()]))\n",
    "\n",
    "# Print the named entities\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee5da6",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043c9780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load pre-trained word embedding model\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Define a list of words to group\n",
    "word_list = [\"cat\", \"dog\", \"horse\", \"apple\", \"banana\", \"orange\"]\n",
    "\n",
    "# Compute the vector representations for each word\n",
    "word_vectors = [model[word] for word in word_list]\n",
    "\n",
    "# Group words with similar vector representations together\n",
    "grouped_words = {}\n",
    "for i, word in enumerate(word_list):\n",
    "    vec = word_vectors[i]\n",
    "    closest_words = [word_list[j] for j in np.argsort([np.dot(vec, word_vectors[k]) for k in range(len(word_list))])[-4:-1]]\n",
    "    grouped_words[word] = closest_words\n",
    "\n",
    "# Print the grouped words\n",
    "for word, group in grouped_words.items():\n",
    "    print(word, \"->\", group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bd47e4",
   "metadata": {},
   "source": [
    "#### Negation handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b025fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_interesting', 'is', 'negative', 'while', 'interesting', 'is', 'positive', '.']\n",
      "{'neg': 0.264, 'neu': 0.286, 'pos': 0.45, 'compound': 0.3818}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Define the sentence with negation\n",
    "sentence = \"Not interesting is negative while interesting is positive.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Perform negation handling\n",
    "negation = False\n",
    "negated_tokens = []\n",
    "for token in tokens:\n",
    "    if token.lower() == \"not\":\n",
    "        negation = True\n",
    "    elif negation:\n",
    "        negated_tokens.append(\"not_\" + token)\n",
    "        negation = False\n",
    "    else:\n",
    "        negated_tokens.append(token)\n",
    "\n",
    "# Print the negated tokens\n",
    "print(negated_tokens)\n",
    "\n",
    "# Perform sentiment analysis using the negated tokens\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "scores = analyzer.polarity_scores(\" \".join(negated_tokens))\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4fa8d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
